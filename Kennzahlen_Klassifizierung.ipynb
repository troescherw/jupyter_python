{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998db410",
   "metadata": {},
   "source": [
    "# Wichtige Kennzahlen für die Klassifizierung\n",
    "Hier werden die wichtigsten Kennzahlen für ein Klassifizierungsmodell vorgestellt.\n",
    "\n",
    "Als Datensatz verwenden wir eine CSV-Datei, die direkt die Ergebnisse des Modells enthält (Feature *PROGNOSE*). In der ersten Spalte sind die realen Werte enthalten (Feature *REALITAET*).\n",
    "\n",
    "0 entspricht hier NEGATIV\n",
    "1 entspricht hier POSITIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/troescherw/datasets/master/daten_kennzahlen_klassifizierung.csv\"\n",
    "df = pd.read_csv(url, delimiter=\";\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d95c63a",
   "metadata": {},
   "source": [
    "Zuerst erstellen wir einen Report für diesen Datensatz. Mit *Seaborn* erstellen wir zudem eine Confusion Matrix mit Hilfe der Funktion *heatmap*. Man beachte hier, dass die Zeilen / Spalten gegenüber üblicher Darstellung in der Literatur vertauscht sind. Spaltenweise sind hier die Prognosen angegeben, zeilenweise die Realität."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce0558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "labels=[\"NEGATIV\", \"POSITIV\"]\n",
    "#sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)[source]\n",
    "cm = confusion_matrix(df.REALITAET, df.PROGNOSE)\n",
    "\n",
    "\n",
    "_=sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = [\"NEGATIV\", \"POSITIV\"]\n",
    "print(classification_report(df.REALITAET, df.PROGNOSE, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75135a",
   "metadata": {},
   "source": [
    "Man kann natürlich jede dieser Kennzahlen (und noch viele weitere) mit Hilfe von Funktionen berechnen lassen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a468dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "TN, FP, FN, TP =  cm[0][0], cm[0][1], cm[1][0],cm[1][1]\n",
    "\n",
    "accuracy = accuracy_score(df.REALITAET, df.PROGNOSE) # Accuracy\n",
    "sensitivity = TP / (TP+FN) # Sensitivität\n",
    "specificity = TN / (TN+FP) # Spezifität\n",
    "precision = precision_score(df.REALITAET, df.PROGNOSE) # Precision\n",
    "balanced_acc = balanced_accuracy_score(df.REALITAET, df.PROGNOSE) # balanced accuracy\n",
    "f1 = f1_score(df.REALITAET, df.PROGNOSE) # F1-Score\n",
    "mcc = matthews_corrcoef(df.REALITAET, df.PROGNOSE) # Mattews Korrelationskoeffizient\n",
    "\n",
    "print(\"Accuracy = \", accuracy)\n",
    "print(\"Sensitivitaet / Recall = \", sensitivity)\n",
    "print(\"Spezifität = \" , specificity)\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Balanced Accuracy = \" , balanced_acc)\n",
    "print(\"F1-Score = \" , f1)\n",
    "print(\"Matthews Korrelationskoeffizient = \" , mcc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
