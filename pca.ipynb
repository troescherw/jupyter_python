{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc952cb",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Wir wollen zuerst die PCA anhand eines einfachen Beispiel mit Hilfe der Linearen Algebra \"händisch\" durchführen. Anschließend verwendenw wir die Klasse *PCA* aus dem Package *sklearn.decomposition*.\n",
    "\n",
    "## Eigenständige Durchführung der PCA\n",
    "\n",
    "Wir berechnen hierzu die Eigenwerte und Eigenvektoren aus der Koeffizientenmatrix.\n",
    "\n",
    "**Definition**: Ein Matrix A multipliziert mit ihrem Eigenvektor $\\overrightarrow{v}$ ist gleich dem Produkt aus einem Skalar $\\lambda$ und dem Eigenvektor:\n",
    "\n",
    "$A\\overrightarrow{v} = \\lambda\\overrightarrow{v}$\n",
    "\n",
    "Hat unser ursprünglicher Datensatz z.B. 5 Dimensionen, so erhalten wir eine 5x5 Koeffizientenmatrix und somit 5 Eigenwerte bzw. Eigenvektoren. Da eine Koeffizientenmatrix immer symmetrisch ist, erhalten wir auch \"sinnvolle\" Werte (also z.B. keine imaginären Zahlen) und die Vektoren stehen auch senkrecht zueinander.\n",
    "\n",
    "Wollen wir von n Dimensionen auf k reduzieren, verwenden wir die k Eigenvektoren der größten k Eigenwerte und multiplizieren damit unsere Daten.\n",
    "\n",
    "Wir wollen dies anhand eines simplen Beispiels zeigen. Wir erstellen dazu zuerst einen, 2-dimensionalen Datensatz und visualisieren die Daten mit einem Scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2994050",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\"X\":[1,3,7,8,10,12,17,25],\n",
    "                   \"Y\":[12,34,67,99,133,159,167,198]})\n",
    "\n",
    "print(df)\n",
    "\n",
    "plt.scatter(df.X, df.Y, color=\"r\")\n",
    "plt.title(\"Beispieldaten für PCA\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c0f39",
   "metadata": {},
   "source": [
    "Nun skalieren wir die Daten und zentrieren diese. Wir subtrahieren von den X- und Y-Werten jeweils deren Mittelwerte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b463a1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_scaled = pd.DataFrame(StandardScaler().fit_transform(df[[\"X\", \"Y\"]]), columns=df.columns)\n",
    "x_mean = df_scaled.X.mean()\n",
    "y_mean = df_scaled.Y.mean()\n",
    "\n",
    "df_centered = pd.DataFrame({\"X\": df_scaled.X-x_mean, \"Y\": df_scaled.Y - y_mean}, columns = df.columns)\n",
    "\n",
    "plt.scatter(df_centered.X, df_centered.Y)\n",
    "plt.axvline(x=0, color='k', linestyle='--')\n",
    "plt.axhline(y=0, color='k', linestyle='--')\n",
    "plt.title(\"Beispieldaten für PCA, skaliert und zentriert\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7219df",
   "metadata": {},
   "source": [
    "Nun berechnen wir die Kovarianz-Matrix sowie die Eigenwerte (eigw) und Eigenvektoren (eigv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdcaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "kovmatr = pd.DataFrame.cov(df_centered)\n",
    "print(\"Kovarianzmatrix:\\n\")\n",
    "print(kovmatr)\n",
    "print()\n",
    "\n",
    "# Berechne Eigenwerte und Eigenvektoren\n",
    "eigw, eigv = np.linalg.eig(kovmatr)\n",
    "\n",
    "print(f\"Eigenwerte: {eigw},\\nEigenvektoren:\\n {eigv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2dd20",
   "metadata": {},
   "source": [
    "Wir multiplizieren unsere zenrierten Daten mit dem Eigenvektor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1dim1 = df_centered @ eigv[1]\n",
    "print(df_1dim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ddbd22",
   "metadata": {},
   "source": [
    "## Vergleich mit Klasse PCA\n",
    "Wir verwenden nun die Klasse *PCA* aus dem Package *sklearn.decomposition*. Als Daten übergeben wir die skalierten Daten. Eine Zentrierung erledigt die *fit*-Methode für uns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3805dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(df_scaled)\n",
    "print(f\"Eigenvektor: {pca.components_}\")\n",
    "print(f\"Eigenwert: {pca.explained_variance_}\")\n",
    "print(\"Tranformierte Daten: \\n\")\n",
    "df_1dim2 = pca.transform(df_scaled)\n",
    "print(df_1dim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48307376",
   "metadata": {},
   "source": [
    "## Beispiel: PCA mit IRIS\n",
    "\n",
    "Wir erstellen ein Modell Random Forest (Klassifikation), um die IRIS-Spezies vorherzusagen. Zuerst mit allen 4 Features, danach mit Hilfe von PCA auf 2 Features reduziert. Wir berechnen jeweils die Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d46c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "X = StandardScaler().fit_transform(data.data)\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle=True,test_size=0.3, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbcfbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest1 = RandomForestClassifier(n_estimators=50).fit(X_train, y_train)\n",
    "print(forest1.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4f44e",
   "metadata": {},
   "source": [
    "Wir reduzieren mit PCA auf nur 2 Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.fit_transform(X_test)\n",
    "\n",
    "forest2 = RandomForestClassifier(n_estimators=100).fit(X_train_pca, y_train)\n",
    "print(forest2.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd7753",
   "metadata": {},
   "source": [
    "Wir reduzieren auf nur 2 Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5cf50",
   "metadata": {},
   "source": [
    "Diese 2 Dimensionen können wir nun auch ganz einfach plotten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "classes = [\"Setosa\", \"Versicolor\", \"Virginica\"]\n",
    "\n",
    "for i in range(3):\n",
    "    data = X_train_pca[y_train==i]\n",
    "    plt.scatter(data[:,0], data[:,1], label=classes[i])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
