{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e1c0ba",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "## Beispiel 1: Grundprinzip\n",
    "\n",
    "Wir erstellen zuerst einen \"künstlichen\" Datensatz und visualisieren diesen mit *matplotlib*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566343d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, y = make_circles(1000, factor=.2, noise=.1)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac0f8f",
   "metadata": {},
   "source": [
    "Wir haben also zwei Klassen, wobei die eine Klasse in Kreisform die andere Klasse einschließt. Es gibt hier keine Möglichkeit, diese beiden Klassen mit einer Entscheidungsgrenze voneinander zu trennen. Deshalb transormieren wir die Daten in den 3-Dimensionalen Raum, hier ganz simpel indem wir jweils die X-Daten quadrieren und die Summe bilden.\n",
    "\n",
    "Anschließend plotten wir die Daten in einem 3D-Plot. Nun können wir uns eine Ebene vorstellen, die diese beiden Klassen voneinander trennt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e58da32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from mpl_toolkits import mplot3d\n",
    "z = (X ** 2).sum(1)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], z, c=y, s=50)\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff4754",
   "metadata": {},
   "source": [
    "Wir versuche nun mit unterschiedlichen Kernels mittels SVM ein Modell zu erstellen. Versuchen wir es zuerst mit dem linearen Kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767b26b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,shuffle=True, test_size=0.3, random_state=42)\n",
    "model_linear = SVC(kernel=\"linear\").fit(X_train, y_train)\n",
    "print(model_linear.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633324ca",
   "metadata": {},
   "source": [
    "Wie erwartet ist die Accuracy recht schlecht! Versuchen wir es mit einem Polynomialen Kernel mit degree=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly = SVC(kernel=\"poly\", degree=2).fit(X_train, y_train)\n",
    "print(model_poly.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e854b8",
   "metadata": {},
   "source": [
    "Jetzt haben wir 100% Genauigkeit! Die Klassen können nun also exakt separiert werden. Versuchen wir es aus Spaß noch mit dem rbf-Kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8585ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rbf = SVC(kernel=\"rbf\").fit(X_train, y_train)\n",
    "print(model_rbf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c6325f",
   "metadata": {},
   "source": [
    "Auch hier 100% Accuracy! Ist auch nicht wirklich überraschend, da sich die Klassen, wie im ersten Plot gezeigt, nicht überlappen und somit mit allen Kernels, die nicht linear sind, gut trennen lassen. Wir können die Entscheidungsgrenze auch mit der Hilfsdatei *plot_decision_boundaries* visualisieren. Diese Datei muss im gleichen Verzeichnis wie diese Jupyter Notebook - Datei liegen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b786a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from plot_decision_boundaries import plot_decision_boundaries\n",
    "plot_decision_boundaries(X_train, y_train, SVC, kernel=\"poly\", degree=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75a545",
   "metadata": {},
   "source": [
    "## Beispiel 2: Entscheidungsgrenzen mit künstlich erzeugten Daten\n",
    "Im 2. Beispiel wollen wir wieder einen künstlich erzeugten Datensatz verwenden. Hier überlappen sich die Objekte der unterschiedlichen Klassen teilweise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(n_samples=500, centers=2, n_features=2, cluster_std=3, random_state=42)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a8e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_decision_boundaries(X, y, SVC, kernel=\"poly\", degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42, shuffle=True)\n",
    "model_lin = SVC(kernel=\"linear\").fit(X_train, y_train)\n",
    "print(model_lin.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_poly = SVC(kernel=\"poly\", degree=3).fit(X_train, y_train)\n",
    "print(model_poly.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e90a7e",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "Die wichtigsten:\n",
    "\n",
    "* **kernel**: ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "* **poly**: Bei Kernel \"poly\" der Grad\n",
    "* **C**: Regularisierungsparameter(\"Penalty\"). Wirkt ggf. Overfitting entgegen. Hoher Wert: \"Harte\" Grenze, kleiner Wert: \"Weiche\" Grenze\n",
    "* **gamma**: {‘scale’, ‘auto’} or float. Wie groß der Einfluss weiter entfernter Punkte ist. Großer Wert: Punkte mit größerem Abstand werden stärker berücksichtigt (näher liegende dafür weniger stark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76e9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klassischer Fall von Overfitting:\n",
    "\n",
    "X, y = make_blobs(n_samples=500, centers=3, n_features=3, cluster_std=3, random_state=42)\n",
    "plot_decision_boundaries(X, y, SVC, kernel=\"rbf\", C=10, gamma=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf939650",
   "metadata": {},
   "source": [
    "## Beispiel 3: Ziffernerkennung MNIST-Datensatz\n",
    "Wir wollen nun mit Hilfe einer SVM handschriftlich geschriebenen Ziffern klassifizieren. Wir verwenden die K-Fold-Cross-Validation im Zusammenhang mit einer Grid-Search, um verschiedene Hyperparameter zu testen. Da die Parameter C und Gamma Fließkommazahlen sind verwenden wir hier die *RandomizedSearchCV*-Klasse.\n",
    "\n",
    "Vor der Verwendung einer SVM sollte man die Daten standardisieren!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2425e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "\n",
    "data = load_digits()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "parameter_grid = {\"kernel\" : [\"linear\", \"poly\", \"rbf\"],\n",
    "                  \"degree\" : [2,3,5],\n",
    "                  \"C\": np.arange(2, 10, 2),\n",
    "                  \"gamma\": np.arange(0.1, 2, 0.2)}\n",
    "\n",
    "\n",
    "grid = RandomizedSearchCV(SVC(), param_distributions = parameter_grid, n_iter=10, scoring=\"accuracy\",\n",
    "                          n_jobs=-1, verbose=3, cv=10, random_state=42)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "\n",
    "model = SVC(kernel=\"poly\", gamma=0.3, degree=3, C=4).fit(X_train, y_train)\n",
    "print(model.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
